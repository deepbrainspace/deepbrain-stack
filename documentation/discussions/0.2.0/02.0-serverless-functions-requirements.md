Date: 2025-04-10
---

We have these specific serverless functions we need to add:

1) A serverless function that would create the Hetzner backup server to do the backups (runs daily to do daily (7 days), weekly (4 weeks) and monthly (12 months) backups). We might need to decide how it would do the backups, whether it should be a custom image or snapshot that already has everything we need and we just spin the snapshot up using the serverless function then just get the server to self-destruct once the backup is done. It should also update our monitoring and alerting mechanism on the status of the backups. I was suggested by Grok3 to use a Google Cloud Function using the Google scheduler to initiate this process, but I am open to other suggestions.

2) We will have interface with Aircall API and Guesty API, we need some kind of serverless functions (or otherwise) to run that would update the Firebase database with this information (ideally on a webhook rather than polling all the time so we get realtime). We need another trigger that would create a vector representation of the information on the Qdrant vectorDB.

3) We need a similar update to the Qdrant vectorDB for when people do any chat messages on the RocketChat. So I suggested we just monitor the RocketChat MongoDB, and anytime we get a new messages there, we can have a process of some sort that updates the Qdrant vectorDB. Grok3 suggested a sync script running on the RocketChat docker compose to do this, but I am open to other ideas also.

4) We need a process that would make a LLM query when someone in the RocketChat asks about any specific domain information on bookings etc (the end client is a BnB business and this setup is to help them manage their business processes and client information/maintenance smoothly). So for instance if they write 'who is checking in today and at what time', we need a process that would go and query the vectorDB for relevant information, then pass the request to the Groq LLM, then pass the response back to the RocketChat for the user. We don't need anything to keep the long term/short term memory as we have the process (3) above that would update the vectorDB with the question and the LLM answer. Grok3 suggested we make a Cloudflare Worker that does this work (but I am open to other ideas/options also).